---
title: "Project"
author: "Kameni"
date: "Sunday, December 21, 2014"
output: word_document
---
```{r}
Sys.setlocale(category = "LC_TIME", locale="English")

time <- format(Sys.time(),"%a, %b %d %Y, %X")


rversion <- R.Version()[[13]]

sysversion <- sessionInfo()[2]
```



```{r, }
library(caret,quietly=TRUE)
library(randomForest,quietly=TRUE) 
```

####1 Data Loading & Explorating
First we want to load the data sets into R 
```{r}

trainingDf<-read.csv("./pml-training.csv", na.strings=c("", "NA", "NULL"))
dim(trainingDf)# Check number of variables and observations

testDf<-read.csv("./pml-testing.csv", na.strings=c("", "NA", "NULL"))
dim(testDf)# Check number of variables and observations


```


####2 Data Processing
in this part we face 2 problems:a) the missing values and b) the fact that the data set contains irrelevant variables.

#####a-) Processing missing values
we decide to Delete columns that contains missing values.

```{r}
trainingDf<-trainingDf[,colSums(is.na(trainingDf)) == 0];dim(trainingDf)
testDf <-testDf[,colSums(is.na(testDf)) == 0];dim(testDf)
```


#####b-) Processing irrelevant variables
As the first 7 columns are non-numeric non-integer, we decide to delete them.
```{r}
trainingDf <-trainingDf[,-c(1:7)];dim(trainingDf)
testDf <-testDf[,-c(1:7)];dim(testDf)
```


##### c) Adapting the columns of training and to testing data set

 We now make sure that both sets have the same number of columns.
 
```{r}
a<-0
colNames<-c()
nomTr<-names(trainingDf);nomTest<-names(testDf)
del<-setdiff( nomTest,trainingDf)
for(i in 1:length(nomTr)){
      if( nomTr[i] %in% nomTest){
          name<- nomTr[i]
          a<-a+1
          colNames[a]<-name
      }      
}
length(colNames)
colNames<-append(colNames,"classe");
trainingDf<-trainingDf[,colNames]
dim(trainingDf)

```


####3 Creating training and cross-validation sets
We have modified the training data(trainingDf). Now we will split the object **trainingDf** into **"train.Sub.trainingDf"(70%)** and **“test.Sub.trainingDf”(30%)**.     
```{r}
set.seed(1442)
data <- createDataPartition(y=trainingDf$classe, p=0.70, list=FALSE)
train.Sub.trainingDf <- trainingDf[data, ];dim(train)
test.Sub.trainingDf<- trainingDf[-data, ];dim(test)

```


#### 4 Predicting Model

predicting with random forest
```{r}
model <- randomForest(classe ~. , data=train.Sub.trainingDf, method="class")

# perform the Prediction
prediction <- predict(model2, test.Sub.trainingDf, type = "class")

# Test the prediction:
confusionMatrix(prediction, test.Sub.trainingDf$classe)

```


####5 Analizing the Error
Decision

As expected, Random Forest algorithm performed better than Decision Trees.
Accuracy for Random Forest model was 0.995 (95% CI: (0.993, 0.997)) compared to 0.739 (95% CI: (0.727, 0.752)) for Decision Tree model. The random Forest model is choosen. The accuracy of the model is 0.995. The expected out-of-sample error is estimated at 0.005, or 0.5%. The expected out-of-sample error is calculated as 1 - accuracy for predictions made against the cross-validation set. Our Test data set comprises 20 cases. With an accuracy above 99% on our cross-validation data, we can expect that very few, or none, of the test samples will be missclassified.


#### 


```{r}
# predict outcome levels on the original Testing data set using Random Forest algorithm
endPrediction <- predict(model, testDf, type="class")
endPrediction
```

